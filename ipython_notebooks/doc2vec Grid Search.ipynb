{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-23 19:30:46.383 - INFO - Starting application on /home/kvassay/data/book-recommender/ratings_Books.csv dataset\n",
      "2016-11-23 19:30:46.384 - INFO - Testing recommender implementation of recommender.doc2vec_recommender \n",
      "2016-11-23 19:31:44.032 - INFO - Selected 100000 users to evaluate their ratings\n",
      "2016-11-23 19:31:47.480 - INFO - Selected dataframe of random 100000 users containing 1381510 entries\n",
      "2016-11-23 19:31:47.483 - INFO - training on users dataset divides on quantile 0.8\n"
     ]
    }
   ],
   "source": [
    "# validation method for recommender systems\n",
    "# computes the RMSE (root mean squared error) for the recommender method on the same dataset\n",
    "# using increasing volume ratio of training/testing dataset\n",
    "# might be used to evaluate how the method improves on increasing volume on testing dataset\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from dummy_recommender import MeanRatingRecommender as Recommender\n",
    "# from knn_recommender_v2 import KnnRecommender as Recommender\n",
    "from recommender.doc2vec_recommender import Doc2VecRecommender as Recommender\n",
    "\n",
    "# DATA_FILE_PATH=\"/home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/ratings_Books.csv\"\n",
    "DATA_FILE_PATH = '/home/kvassay/data/book-recommender/ratings_Books.csv'\n",
    "\n",
    "SAMPLED_USERS = 100000\n",
    "USER_IS_ROBOT_THRESHOLD = 100\n",
    "\n",
    "# slices to use for testing methods improvements on increasing amount of testing data\n",
    "SLICING_INTERVAL = 5\n",
    "\n",
    "# select how many times the evaluation will split data and test\n",
    "# selecting 1 means one split with fold on SLICING_INTERVAL-1/SLICING_INTERVAL timestamp for every user\n",
    "# can automatically test a development of model performance on increasing amount of training data\n",
    "SLICING_RUNS = 1\n",
    "SPLIT_Q=0.8\n",
    "method_name = Recommender.__module__\n",
    "\n",
    "# logging init:\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(20)\n",
    "\n",
    "FORMAT = '%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s'\n",
    "DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "formatter = logging.Formatter(fmt=FORMAT, datefmt=DATE_FORMAT)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "data_file = DATA_FILE_PATH\n",
    "\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "logger.info(\"Starting application on %s dataset\" % data_file)\n",
    "logger.info(\"Testing recommender implementation of %s \" % method_name)\n",
    "\n",
    "with open(data_file, \"r\") as f:\n",
    "    df = pd.read_csv(data_file, names=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
    "\n",
    "grouped_users = df.groupby([\"user\"]).count()\n",
    "n_review_users = grouped_users[grouped_users[\"item\"] >= SLICING_INTERVAL][\"item\"].keys()\n",
    "eval_users = np.random.choice(n_review_users.unique(), SAMPLED_USERS)\n",
    "logger.info(\"Selected %s users to evaluate their ratings\" % eval_users.__len__())\n",
    "\n",
    "eval_dataframe = df[df['user'].isin(eval_users)]\n",
    "logger.info(\"Selected dataframe of random %s users containing %s entries\" %\n",
    "            (eval_users.__len__(), eval_dataframe.__len__()))\n",
    "\n",
    "\n",
    "\n",
    "training_frame = pd.DataFrame(columns=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
    "testing_frame = pd.DataFrame(columns=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
    "\n",
    "\n",
    "quantile = SPLIT_Q\n",
    "logger.info(\"training on users dataset divides on quantile %s\" % quantile)\n",
    "\n",
    "# TODO: later compare recommender results with using dataset having only data newer than from 2013\n",
    "\n",
    "for user in eval_users:\n",
    "    user_reviews = eval_dataframe[eval_dataframe['user'] == user]\n",
    "\n",
    "    if len(user_reviews) >= USER_IS_ROBOT_THRESHOLD:\n",
    "        # do not include users having more than threshold ratings\n",
    "        continue\n",
    "\n",
    "    # value dividing reviews of a user to training and testing\n",
    "    slicing_timestamp = user_reviews[\"timestamp\"].quantile(q=quantile)\n",
    "\n",
    "    training_user_data = user_reviews[user_reviews[\"timestamp\"] < slicing_timestamp]\n",
    "    training_frame = training_frame.append(training_user_data)\n",
    "\n",
    "    testing_user_data = user_reviews[user_reviews[\"timestamp\"] >= slicing_timestamp]\n",
    "    testing_frame = testing_frame.append(testing_user_data)\n",
    "\n",
    "logger.info(\"training dataframe size: %s\" % training_frame.__len__())\n",
    "logger.info(\"testing dataframe size: %s\" % testing_frame.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(recommender_instance):\n",
    "    recommender_instance.fit(training_frame)\n",
    "    logger.info(\"Recommender method has fit on %s entries\" % training_frame.__len__())\n",
    "\n",
    "    # aggregated difference of recommender predicted rating against the real rating\n",
    "    delta_sum = 0\n",
    "    len_diff = 0\n",
    "    for index, entry in testing_frame.iterrows():\n",
    "        expected_score = entry[\"rating\"]\n",
    "        actual_score = recommender_instance.predict(entry[\"user\"], entry[\"item\"])\n",
    "\n",
    "        if actual_score is not None:\n",
    "            delta_sum += math.fabs(expected_score - actual_score)\n",
    "        else:\n",
    "            len_diff += 1\n",
    "\n",
    "    logger.info(\"Recommender method has predicted %s ratings\" % (testing_frame.__len__()-len_diff))\n",
    "    logger.info('Recommender failed to predict %s ratings', len_diff)\n",
    "\n",
    "    delta = float(delta_sum) / (testing_frame.__len__() - len_diff)\n",
    "    mean_rating = float(testing_frame[\"rating\"].mean())\n",
    "\n",
    "    logger.info(\"Testing data mean rating: %s\" % mean_rating)\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Method %s average error delta %s\" % (method_name, delta))\n",
    "    logger.info(\"\")\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_cfg(cfgs):\n",
    "    cfg=dict()\n",
    "    for key in cfgs:\n",
    "        num_options=len(cfgs[key])\n",
    "        cfg[key]=np.random.choice(cfgs[key],1)[0]\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CFGS={'num_epochs':[1,3,5,7,10,20], 'alpha':[0.1,0.025,0.15,0.3], 'min_alpha':[0.025,0.01,0.001,0.5], 'dm':[1], 'size':[10,30,50,70,100,150,200,300], 'window':range(1,15,1), 'min_count':[1], 'negative':[0,5,10]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "TIME_LIMIT_S=3600\n",
    "logger.setLevel(40)\n",
    "best_cfg=None\n",
    "best_score=10000\n",
    "start=time.time()\n",
    "\n",
    "n_iter=1\n",
    "score_improvements=list()\n",
    "while time.time()-start < TIME_LIMIT_S:\n",
    "    cfg=get_random_cfg(CFGS)\n",
    "    rec=Recommender(cfg)\n",
    "    score=evaluate(rec)\n",
    "    if score<best_score:\n",
    "        best_cfg=cfg\n",
    "        best_score=score\n",
    "        score_improvements.append(best_score)\n",
    "        print('Found new best config in iter No. '+str(n_iter)+' ___________________________')\n",
    "        print('Best cfg: '+str(best_cfg))\n",
    "        print('Best score: '+str(best_score))\n",
    "    n_iter+=1\n",
    "print('Score improvemetns: '+str(score_improvements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(score_improvements)\n",
    "plt.title('Grid search score improvements')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
