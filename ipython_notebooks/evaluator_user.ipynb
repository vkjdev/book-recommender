{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# TODO: fill your path to Evaluator class\n",
    "# pickled files uploaded to:\n",
    "# https://drive.google.com/drive/folders/0B_42L5-Ve7j2Rk04cUZIMkRRaFk?usp=sharing\n",
    "sys.path.append(\"/home/michal/Documents/Misc/recommenders/vcs/book-recommender/recommender\")\n",
    "from evaluator2 import Evaluator\n",
    "\n",
    "from dummy_recommender import MeanRatingRecommender as Recommender\n",
    "# from knn_recommender_v2 import KnnRecommender as Recommender\n",
    "# from doc2vec_recommender import Doc2VecRecommender as Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PICKLE_USER_RANGE = [50, 50]\n",
    "\n",
    "PICKLE_DATA = {\"train_data\": \"/home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/%s_%s_ratings_train.dat\"\n",
    "                             % (PICKLE_USER_RANGE[0], PICKLE_USER_RANGE[1]),\n",
    "               \"test_data\": \"/home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/%s_%s_ratings_test.dat\"\n",
    "                            % (PICKLE_USER_RANGE[0], PICKLE_USER_RANGE[1])}\n",
    "\n",
    "tested_recommender = Recommender()\n",
    "evaluator = Evaluator(pickle_boost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tested_recommender = Recommender()\n",
    "evaluator = Evaluator(pickle_boost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-05 18:04:48.085 - INFO - Testing recommender implementation of dummy_recommender \n",
      "2016-12-05 18:04:48.087 - INFO - /home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/50_50_ratings_train.dat\n",
      "2016-12-05 18:04:48.222 - INFO - Loaded train set having 28888 reviews\n",
      "2016-12-05 18:04:48.265 - INFO - Loaded test set having 8562 reviews\n",
      "2016-12-05 18:04:48.267 - INFO - Dataset contains reviews of 746 users\n",
      "2016-12-05 18:04:48.268 - INFO - Loaded pickled train and test data frames\n",
      "2016-12-05 18:04:48.268 - INFO - Starting recommender fit\n",
      "2016-12-05 18:04:48.269 - INFO - Recommender has fit\n",
      "2016-12-05 18:04:48.270 - INFO - Starting testing - gathering recommender predictions\n",
      "2016-12-05 18:04:48.965 - INFO - Recommender method has predicted 8562 ratings\n",
      "2016-12-05 18:04:48.966 - INFO - Recommender failed to predict 0 ratings\n",
      "2016-12-05 18:04:48.987 - INFO - Recommender scores: {'recall': 0.7907031067507592, 'mae': 0.89021256715720631, 'precision': 1.0, 'f_score': 0.8831202713279416, 'rmse': 1.1129821511877067}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f_score': 0.8831202713279416,\n",
       " 'mae': 0.89021256715720631,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.7907031067507592,\n",
       " 'rmse': 1.1129821511877067}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tested_recommender, pickled_train_filepath=PICKLE_DATA[\"train_data\"], pickled_test_filepath=PICKLE_DATA[\"test_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f_score': 0.8831202713279416,\n",
       " 'mae': 0.89021256715720631,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.7907031067507592,\n",
       " 'rmse': 1.1129821511877067}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_recent_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1129821511877067"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_recent_results(method=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-05 18:05:06.184 - INFO - Pattern library is not installed, lemmatization won't be available.\n",
      "2016-12-05 18:05:06.185 - INFO - Could not import Theano, will use standard float for default ShardedCorpus dtype.\n",
      "2016-12-05 18:05:06.191 - INFO - 'pattern' package not found; tag filters are not available for English\n",
      "2016-12-05 18:05:06.195 - INFO - Initializing Doc2Vec Recommender...\n"
     ]
    }
   ],
   "source": [
    "from doc2vec_recommender import Doc2VecRecommender as Recommender\n",
    "tested_recommender = Recommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PICKLE_USER_RANGE = [20, 50]\n",
    "\n",
    "PICKLE_DATA = {\"train_data\": \"/home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/%s_%s_ratings_train.dat\"\n",
    "                             % (PICKLE_USER_RANGE[0], PICKLE_USER_RANGE[1]),\n",
    "               \"test_data\": \"/home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/%s_%s_ratings_test.dat\"\n",
    "                            % (PICKLE_USER_RANGE[0], PICKLE_USER_RANGE[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-05 18:05:12.039 - INFO - Testing recommender implementation of doc2vec_recommender \n",
      "2016-12-05 18:05:12.039 - INFO - /home/michal/Documents/Misc/recommenders/vcs/book-recommender/data/20_50_ratings_train.dat\n",
      "2016-12-05 18:05:19.816 - INFO - Loaded train set having 1684154 reviews\n",
      "2016-12-05 18:05:22.888 - INFO - Loaded test set having 584279 reviews\n",
      "2016-12-05 18:05:22.974 - INFO - Dataset contains reviews of 76924 users\n",
      "2016-12-05 18:05:22.975 - INFO - Loaded pickled train and test data frames\n",
      "2016-12-05 18:05:22.975 - INFO - Starting recommender fit\n",
      "2016-12-05 18:05:22.976 - INFO - Extracting User:[items] dict from dataframe.\n",
      "2016-12-05 18:05:23.126 - INFO - Transformed 100000 rows to user:items dict. Transforming last 100000 rows took 0.149384975433s...\n",
      "2016-12-05 18:05:23.267 - INFO - Transformed 200000 rows to user:items dict. Transforming last 100000 rows took 0.141320943832s...\n",
      "2016-12-05 18:05:23.411 - INFO - Transformed 300000 rows to user:items dict. Transforming last 100000 rows took 0.144030809402s...\n",
      "2016-12-05 18:05:23.557 - INFO - Transformed 400000 rows to user:items dict. Transforming last 100000 rows took 0.145989894867s...\n",
      "2016-12-05 18:05:23.703 - INFO - Transformed 500000 rows to user:items dict. Transforming last 100000 rows took 0.146323204041s...\n",
      "2016-12-05 18:05:23.842 - INFO - Transformed 600000 rows to user:items dict. Transforming last 100000 rows took 0.138839006424s...\n",
      "2016-12-05 18:05:23.987 - INFO - Transformed 700000 rows to user:items dict. Transforming last 100000 rows took 0.144703865051s...\n",
      "2016-12-05 18:05:24.132 - INFO - Transformed 800000 rows to user:items dict. Transforming last 100000 rows took 0.144866943359s...\n",
      "2016-12-05 18:05:24.273 - INFO - Transformed 900000 rows to user:items dict. Transforming last 100000 rows took 0.141516923904s...\n",
      "2016-12-05 18:05:24.416 - INFO - Transformed 1000000 rows to user:items dict. Transforming last 100000 rows took 0.142784118652s...\n",
      "2016-12-05 18:05:24.555 - INFO - Transformed 1100000 rows to user:items dict. Transforming last 100000 rows took 0.138710975647s...\n",
      "2016-12-05 18:05:24.685 - INFO - Transformed 1200000 rows to user:items dict. Transforming last 100000 rows took 0.130088090897s...\n",
      "2016-12-05 18:05:24.811 - INFO - Transformed 1300000 rows to user:items dict. Transforming last 100000 rows took 0.1261510849s...\n",
      "2016-12-05 18:05:24.948 - INFO - Transformed 1400000 rows to user:items dict. Transforming last 100000 rows took 0.137189865112s...\n",
      "2016-12-05 18:05:25.093 - INFO - Transformed 1500000 rows to user:items dict. Transforming last 100000 rows took 0.145265102386s...\n",
      "2016-12-05 18:05:25.252 - INFO - Transformed 1600000 rows to user:items dict. Transforming last 100000 rows took 0.158441066742s...\n",
      "2016-12-05 18:05:25.398 - INFO - Transforming training data rows to Tagged Documents for Gensim...\n",
      "2016-12-05 18:05:25.793 - INFO - Training Doc2Vec model on 76461 examples.\n",
      "2016-12-05 18:05:25.793 - WARNING - consider setting layer size to a multiple of 4 for greater performance\n",
      "2016-12-05 18:05:25.794 - INFO - collecting all words and their counts\n",
      "2016-12-05 18:05:25.795 - INFO - PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-12-05 18:05:25.928 - INFO - PROGRESS: at example #10000, processed 220928 words (1663189/s), 140279 word types, 10000 tags\n",
      "2016-12-05 18:05:26.053 - INFO - PROGRESS: at example #20000, processed 440642 words (1764956/s), 234935 word types, 20000 tags\n",
      "2016-12-05 18:05:26.178 - INFO - PROGRESS: at example #30000, processed 661566 words (1790204/s), 312269 word types, 30000 tags\n",
      "2016-12-05 18:05:26.302 - INFO - PROGRESS: at example #40000, processed 881498 words (1774162/s), 379288 word types, 40000 tags\n",
      "2016-12-05 18:05:26.622 - INFO - PROGRESS: at example #50000, processed 1102161 words (691443/s), 438987 word types, 50000 tags\n",
      "2016-12-05 18:05:26.721 - INFO - PROGRESS: at example #60000, processed 1322181 words (2236065/s), 493093 word types, 60000 tags\n",
      "2016-12-05 18:05:26.825 - INFO - PROGRESS: at example #70000, processed 1541845 words (2143837/s), 543268 word types, 70000 tags\n",
      "2016-12-05 18:05:26.896 - INFO - collected 573552 word types and 76461 unique tags from a corpus of 76461 examples and 1683514 words\n",
      "2016-12-05 18:05:28.980 - INFO - min_count=1 retains 573552 unique words (drops 0)\n",
      "2016-12-05 18:05:28.980 - INFO - min_count leaves 1683514 word corpus (100% of original 1683514)\n",
      "2016-12-05 18:05:29.966 - INFO - deleting the raw counts dictionary of 573552 items\n",
      "2016-12-05 18:05:29.979 - INFO - sample=0 downsamples 0 most-common words\n",
      "2016-12-05 18:05:29.980 - INFO - downsampling leaves estimated 1683514 word corpus (100.0% of prior 1683514)\n",
      "2016-12-05 18:05:29.980 - INFO - estimated required memory for 573552 words and 30 dimensions: 563606400 bytes\n",
      "2016-12-05 18:05:30.721 - INFO - constructing a huffman tree from 573552 words\n",
      "2016-12-05 18:05:48.083 - INFO - built huffman tree with maximum node depth 21\n",
      "2016-12-05 18:05:48.323 - INFO - resetting layer weights\n",
      "2016-12-05 18:05:53.343 - INFO - training model with 4 workers on 573552 vocabulary and 30 features, using sg=1 hs=1 sample=0 negative=0\n",
      "2016-12-05 18:05:53.343 - INFO - expecting 76461 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-05 18:05:54.359 - INFO - PROGRESS: at 7.70% examples, 670196 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:05:55.375 - INFO - PROGRESS: at 15.65% examples, 679491 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:05:56.388 - INFO - PROGRESS: at 23.38% examples, 676389 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:05:57.395 - INFO - PROGRESS: at 31.07% examples, 675895 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:05:58.399 - INFO - PROGRESS: at 38.92% examples, 677857 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:05:59.409 - INFO - PROGRESS: at 46.74% examples, 678602 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:06:00.430 - INFO - PROGRESS: at 54.34% examples, 675106 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:06:01.432 - INFO - PROGRESS: at 62.53% examples, 680552 words/s, in_qsize 8, out_qsize 0\n",
      "2016-12-05 18:06:02.466 - INFO - PROGRESS: at 69.89% examples, 674410 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:06:03.469 - INFO - PROGRESS: at 77.00% examples, 669479 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:06:04.470 - INFO - PROGRESS: at 84.36% examples, 667408 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:06:05.486 - INFO - PROGRESS: at 91.83% examples, 665723 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-05 18:06:06.419 - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-05 18:06:06.445 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-05 18:06:06.455 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-05 18:06:06.459 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-05 18:06:06.462 - INFO - training on 8417570 raw words (8799875 effective words) took 13.1s, 670953 effective words/s\n",
      "2016-12-05 18:06:06.462 - INFO - Training finished with duration: 40.6689429283s...\n",
      "2016-12-05 18:06:06.487 - INFO - Recommender has fit\n",
      "2016-12-05 18:06:06.489 - INFO - Starting testing - gathering recommender predictions\n",
      "2016-12-05 18:07:37.301 - INFO - Recommender method has predicted 555614 ratings\n",
      "2016-12-05 18:07:37.302 - INFO - Recommender failed to predict 28665 ratings\n",
      "2016-12-05 18:07:38.325 - INFO - Recommender scores: {'recall': 0.8102300418199094, 'mae': 1.1749577944400249, 'precision': 0.5582255453600145, 'f_score': 0.6610241665117617, 'rmse': 1.3926978686371398}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f_score': 0.6610241665117617,\n",
       " 'mae': 1.1749577944400249,\n",
       " 'precision': 0.5582255453600145,\n",
       " 'recall': 0.8102300418199094,\n",
       " 'rmse': 1.3926978686371398}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tested_recommender, pickled_train_filepath=PICKLE_DATA[\"train_data\"], pickled_test_filepath=PICKLE_DATA[\"test_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3926978686371398"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_recent_results(method=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have fun\n"
     ]
    }
   ],
   "source": [
    "print(\"Have fun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
